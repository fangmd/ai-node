# backend-ai-api (delta)

## Purpose

Delta spec for chat-message-session: Chat endpoint SHALL accept optional sessionId and SHALL persist sessions and messages; response remains UI message stream.

## MODIFIED Requirements

### Requirement: Chat endpoint

The backend SHALL provide `POST /api/ai/chat` that accepts a JSON body with a `messages` array in **UIMessage format** (AI SDK UI message format: each item has `id`, `role`, and `parts`) and an optional `sessionId` (string, Session Snowflake id). When `sessionId` is absent, the backend SHALL create a new Session for the authenticated user upon the first user message, persist user and assistant messages as specified by message-storage, and SHALL provide the new sessionId to the client (e.g. via stream metadata or first chunk). When `sessionId` is present, the backend SHALL verify the session belongs to the authenticated user (user_id match); if valid, SHALL append and persist messages to that session; if invalid, SHALL respond with an error (e.g. 403 or 404). The endpoint SHALL convert UIMessages to model messages (e.g. via `convertToModelMessages`) and SHALL call the ai-sdk `streamText` with an OpenAI-compatible model, a **web_search** tool (e.g. provider `webSearch`), and **local tools** (see Local tools). The response SHALL be a **UI message stream** (e.g. `toUIMessageStreamResponse` with `originalMessages`), not plain text SSE. The stream SHALL carry text deltas, tool-call and tool-result parts so that clients (e.g. useChat) can render message parts. The endpoint SHALL use the ai-sdk to call an OpenAI-compatible LLM (e.g. Responses API). The **provider** and **model** used SHALL be determined by environment variables (see OpenAI config via environment). Message ids SHALL be generated by the backend (Snowflake BigInt, serialized as string in the stream) when persisting; client-provided message ids in the request MAY be ignored for persistence.

#### Scenario: chat streams UI message reply

- **WHEN** a client sends `POST /api/ai/chat` with a valid body `{ "messages": [ <UIMessage> ] }` (e.g. a user message with `role: "user"` and `parts: [{ type: "text", text: "Hello" }]`)
- **THEN** the response is a UI message stream (e.g. `Content-Type` and format compatible with AI SDK UI message stream), HTTP status 200, and the body delivers assistant reply content including text deltas and any tool-call/tool-result parts (e.g. web_search, local tools) until the stream ends

#### Scenario: chat with sessionId appends to session

- **WHEN** a client sends `POST /api/ai/chat` with a valid body including `sessionId` equal to a session owned by the authenticated user
- **THEN** the backend SHALL persist user and assistant messages to that session and the response SHALL be a UI message stream as above

#### Scenario: chat without sessionId creates new session

- **WHEN** a client sends `POST /api/ai/chat` with a valid body and no sessionId
- **THEN** the backend SHALL create a new Session for the user, persist messages to it, stream the reply, and SHALL communicate the new sessionId to the client (e.g. in stream or response)

#### Scenario: chat fails when sessionId not owned

- **WHEN** a client sends `POST /api/ai/chat` with a sessionId that does not exist or whose user_id does not match the authenticated user
- **THEN** the backend SHALL respond with an error (e.g. 403 or 404) and SHALL NOT run the chat or persist messages

#### Scenario: chat fails when upstream config missing

- **WHEN** the required configuration for the selected provider (from `AI_PROVIDER` or default) is not set (e.g. for `openai`, `OPENAI_BASE_URL` or `OPENAI_API_KEY` is missing; for `deepseek`, `DEEPSEEK_BASE_URL` or `DEEPSEEK_API_KEY` is missing) and a client sends `POST /api/ai/chat`
- **THEN** the backend SHALL respond with an error (e.g. 503 or 400) using a non-streaming response (e.g. JSON with unified fail format) and a message indicating configuration is missing
