## MODIFIED Requirements

### Requirement: Chat endpoint

The backend SHALL provide `POST /api/ai/chat` that accepts a JSON body with a `messages` array in **UIMessage format** (AI SDK UI message format: each item has `id`, `role`, and `parts`) and an optional `sessionId` (string, Session Snowflake id). The body MAY also include an optional `llmConfigId` (string, LLM config Snowflake id) to select and/or update the model configuration bound to the session.

When `sessionId` is absent, the backend SHALL create a new Session for the authenticated user upon the first user message, persist user and assistant messages as specified by message-storage, bind the session to a selected LLM config id (`llmConfigId`), and SHALL provide the new sessionId to the client (e.g. via stream metadata or response header).

When `sessionId` is present, the backend SHALL verify the session belongs to the authenticated user (user_id match); if valid, SHALL append and persist messages to that session; if invalid, SHALL respond with an error (e.g. 403 or 404).

Model selection and binding rules:
- If request includes `llmConfigId`, the backend SHALL verify that the referenced config belongs to the authenticated user; if valid, the backend SHALL update the session's bound `llmConfigId` to the provided value and use that config for this chat call.
- If request does not include `llmConfigId`, the backend SHALL use the session's currently bound `llmConfigId` if present.
- If the session has no bound `llmConfigId`, the backend SHALL resolve the authenticated user's default LLM config and use it; in this case, the backend SHALL also update the session to bind to that default `llmConfigId` (binding补齐).
- If no usable LLM config can be resolved for the user, the backend SHALL reject the request with an error indicating the user must configure an LLM config before chatting.

The endpoint SHALL convert UIMessages to model messages (e.g. via `convertToModelMessages`) and SHALL call the ai-sdk `streamText` with an OpenAI-compatible model, a **web_search** tool (e.g. provider `webSearch`), and **local tools** (see Local tools). The response SHALL be a **UI message stream** (e.g. `toUIMessageStreamResponse` with `originalMessages`), not plain text SSE. The stream SHALL carry text deltas, tool-call and tool-result parts so that clients (e.g. useChat) can render message parts. The endpoint SHALL use the ai-sdk to call an OpenAI-compatible LLM (e.g. Responses API). Message ids SHALL be generated by the backend (Snowflake BigInt, serialized as string in the stream) when persisting; client-provided message ids in the request MAY be ignored for persistence.

#### Scenario: chat streams UI message reply
- **WHEN** a client sends `POST /api/ai/chat` with a valid body `{ "messages": [ <UIMessage> ] }` (e.g. a user message with `role: "user"` and `parts: [{ type: "text", text: "Hello" }]`) and the user has a usable LLM config (bound or default)
- **THEN** the response is a UI message stream (e.g. `Content-Type` and format compatible with AI SDK UI message stream), HTTP status 200, and the body delivers assistant reply content including text deltas and any tool-call/tool-result parts (e.g. web_search, local tools) until the stream ends

#### Scenario: chat with sessionId appends to session
- **WHEN** a client sends `POST /api/ai/chat` with a valid body including `sessionId` equal to a session owned by the authenticated user
- **THEN** the backend SHALL persist user and assistant messages to that session and the response SHALL be a UI message stream as above

#### Scenario: chat without sessionId creates new session
- **WHEN** a client sends `POST /api/ai/chat` with a valid body and no sessionId, and includes a valid `llmConfigId`
- **THEN** the backend SHALL create a new Session for the user, bind it to `llmConfigId`, persist messages to it, stream the reply, and SHALL communicate the new sessionId to the client (e.g. in stream or response)

#### Scenario: chat without llmConfigId binds to user default
- **WHEN** a client sends `POST /api/ai/chat` with a valid body and a `sessionId` for which the session has no bound `llmConfigId`, and the user has a default LLM config
- **THEN** the backend SHALL use the user's default LLM config for this chat call and SHALL update the session to bind to that default `llmConfigId`

#### Scenario: chat fails when sessionId not owned
- **WHEN** a client sends `POST /api/ai/chat` with a sessionId that does not exist or whose user_id does not match the authenticated user
- **THEN** the backend SHALL respond with an error (e.g. 403 or 404) and SHALL NOT run the chat or persist messages

#### Scenario: chat fails when llmConfigId not owned
- **WHEN** a client sends `POST /api/ai/chat` with a `llmConfigId` that does not belong to the authenticated user
- **THEN** the backend SHALL respond with an error (e.g. 403 or 404) and SHALL NOT run the chat or update session binding

#### Scenario: chat fails when user has no llm config
- **WHEN** a client sends `POST /api/ai/chat` and the user has no usable LLM config (no bound config and no default config)
- **THEN** the backend SHALL respond with an error indicating the user must configure an LLM config before chatting

### Requirement: Sessions endpoint

The backend SHALL provide `GET /api/ai/sessions` that returns the current user's chat sessions. The endpoint SHALL require a valid JWT (via `jwtAuth` middleware). The response SHALL contain only sessions whose user_id matches the authenticated user, ordered by update_time descending. Each session item SHALL include: id (string, Session Snowflake id), title (optional string), updateTime (ISO string), and llmConfigId (optional string, session-bound LLM config id).

#### Scenario: list sessions returns user sessions only
- **WHEN** a client sends `GET /api/ai/sessions` with valid JWT
- **THEN** the response SHALL contain only sessions whose user_id matches the authenticated user, ordered (e.g. by update_time descending)

#### Scenario: list sessions includes llmConfigId
- **WHEN** a client sends `GET /api/ai/sessions` with valid JWT
- **THEN** each returned session item includes `llmConfigId` when the session is bound to a config, otherwise omits it (or returns null)

## REMOVED Requirements

### Requirement: OpenAI config via environment
**Reason**: Chat LLM configuration is no longer sourced from process env defaults; it is sourced from user-owned LLM configs and the session binding (`llmConfigId`).
**Migration**: Configure at least one LLM config via the new settings LLM config APIs/UI and set a default; clients SHOULD ensure chats are created with a bound `llmConfigId` or rely on user default binding.

