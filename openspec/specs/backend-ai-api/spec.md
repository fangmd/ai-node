# backend-ai-api

## Purpose

AI API route group under `/api/ai`: Hono sub-app mount and initial hello endpoint; future AI endpoints (chat, completions, etc.) live under this group.

## Requirements

### Requirement: AI route group

The backend SHALL expose an AI API route group at path prefix `/api/ai`. All AI-related endpoints SHALL be mounted under this prefix via a Hono sub-app (e.g. `app.route('/api/ai', ai)`).

#### Scenario: AI group mounted

- **WHEN** the backend is running
- **THEN** requests to paths under `/api/ai/*` are handled by the AI route group

### Requirement: AI hello endpoint

The backend SHALL provide `GET /api/ai/hello` that returns a success response using the existing unified response format (code 200, msg "success", data).

#### Scenario: hello returns success

- **WHEN** a client sends `GET /api/ai/hello`
- **THEN** the response body is `{ "code": 200, "msg": "success", "data": <object> }` and HTTP status is 200

### Requirement: Chat endpoint

The backend SHALL provide `POST /api/ai/chat` that accepts a JSON body with a `messages` array in **UIMessage format** (AI SDK UI message format: each item has `id`, `role`, and `parts`) and an optional `sessionId` (string, Session Snowflake id). The body MAY also include an optional `llmConfigId` (string, LLM config Snowflake id) to select and/or update the model configuration bound to the session.

When `sessionId` is absent, the backend SHALL create a new Session for the authenticated user upon the first user message, persist user and assistant messages as specified by message-storage, bind the session to a selected LLM config id (`llmConfigId`), and SHALL provide the new sessionId to the client (e.g. via stream metadata or response header).

When `sessionId` is present, the backend SHALL verify the session belongs to the authenticated user (user_id match); if valid, SHALL append and persist messages to that session; if invalid, SHALL respond with an error (e.g. 403 or 404).

Model selection and binding rules:
- If request includes `llmConfigId`, the backend SHALL verify that the referenced config belongs to the authenticated user; if valid, the backend SHALL update the session's bound `llmConfigId` to the provided value and use that config for this chat call.
- If request does not include `llmConfigId`, the backend SHALL use the session's currently bound `llmConfigId` if present.
- If the session has no bound `llmConfigId`, the backend SHALL resolve the authenticated user's default LLM config and use it; in this case, the backend SHALL also update the session to bind to that default `llmConfigId` (binding补齐).
- If no usable LLM config can be resolved for the user, the backend SHALL reject the request with an error indicating the user must configure an LLM config before chatting.

The endpoint SHALL convert UIMessages to model messages (e.g. via `convertToModelMessages`) and SHALL call the ai-sdk `streamText` with an OpenAI-compatible model, a **web_search** tool (e.g. provider `webSearch`), and **local tools** (see Local tools). The response SHALL be a **UI message stream** (e.g. `toUIMessageStreamResponse` with `originalMessages`), not plain text SSE. The stream SHALL carry text deltas, tool-call and tool-result parts so that clients (e.g. useChat) can render message parts. The endpoint SHALL use the ai-sdk to call an OpenAI-compatible LLM (e.g. Responses API). Message ids SHALL be generated by the backend (Snowflake BigInt, serialized as string in the stream) when persisting; client-provided message ids in the request MAY be ignored for persistence.

#### Scenario: chat streams UI message reply
- **WHEN** a client sends `POST /api/ai/chat` with a valid body `{ "messages": [ <UIMessage> ] }` (e.g. a user message with `role: "user"` and `parts: [{ type: "text", text: "Hello" }]`) and the user has a usable LLM config (bound or default)
- **THEN** the response is a UI message stream (e.g. `Content-Type` and format compatible with AI SDK UI message stream), HTTP status 200, and the body delivers assistant reply content including text deltas and any tool-call/tool-result parts (e.g. web_search, local tools) until the stream ends

#### Scenario: chat with sessionId appends to session
- **WHEN** a client sends `POST /api/ai/chat` with a valid body including `sessionId` equal to a session owned by the authenticated user
- **THEN** the backend SHALL persist user and assistant messages to that session and the response SHALL be a UI message stream as above

#### Scenario: chat without sessionId creates new session
- **WHEN** a client sends `POST /api/ai/chat` with a valid body and no sessionId, and includes a valid `llmConfigId`
- **THEN** the backend SHALL create a new Session for the user, bind it to `llmConfigId`, persist messages to it, stream the reply, and SHALL communicate the new sessionId to the client (e.g. in stream or response)

#### Scenario: chat without llmConfigId binds to user default
- **WHEN** a client sends `POST /api/ai/chat` with a valid body and a `sessionId` for which the session has no bound `llmConfigId`, and the user has a default LLM config
- **THEN** the backend SHALL use the user's default LLM config for this chat call and SHALL update the session to bind to that default `llmConfigId`

#### Scenario: chat fails when sessionId not owned
- **WHEN** a client sends `POST /api/ai/chat` with a sessionId that does not exist or whose user_id does not match the authenticated user
- **THEN** the backend SHALL respond with an error (e.g. 403 or 404) and SHALL NOT run the chat or persist messages

#### Scenario: chat fails when llmConfigId not owned
- **WHEN** a client sends `POST /api/ai/chat` with a `llmConfigId` that does not belong to the authenticated user
- **THEN** the backend SHALL respond with an error (e.g. 403 or 404) and SHALL NOT run the chat or update session binding

#### Scenario: chat fails when user has no llm config
- **WHEN** a client sends `POST /api/ai/chat` and the user has no usable LLM config (no bound config and no default config)
- **THEN** the backend SHALL respond with an error indicating the user must configure an LLM config before chatting

### Requirement: DeepSeek provider dependency

The backend SHALL use the official DeepSeek provider from the AI SDK when `AI_PROVIDER=deepseek`. The project SHALL add the dependency with `pnpm add @ai-sdk/deepseek` (in the backend app). The chat endpoint SHALL use `@ai-sdk/deepseek` to create the DeepSeek language model instance, not a generic OpenAI-compatible client with a DeepSeek base URL.

#### Scenario: DeepSeek provider package installed

- **WHEN** the backend is built or installed
- **THEN** `@ai-sdk/deepseek` SHALL be listed as a dependency in the backend package (e.g. `apps/backend/package.json`)

#### Scenario: DeepSeek selected uses official provider

- **WHEN** `AI_PROVIDER=deepseek` and DeepSeek config is set and a client sends `POST /api/ai/chat`
- **THEN** the chat endpoint SHALL call the LLM via the provider created from `@ai-sdk/deepseek` (e.g. `createDeepSeek` or equivalent)

### Requirement: Local tools

The backend SHALL implement **local tools** (server-side tools callable by the model via the chat stream) under the **ai/tools** directory (e.g. `apps/backend/src/ai/tools/`). Local tools SHALL be registered with `streamText` together with provider tools (e.g. web_search). At least one local tool SHALL be available: **get_server_ip**, which returns the current server IP address.

#### Scenario: get_server_ip returns fixed value

- **WHEN** the model invokes the get_server_ip tool during a chat
- **THEN** the tool SHALL return the value `0.0.0.0` (fixed; no real network lookup)

#### Scenario: local tools under ai/tools

- **WHEN** the backend implements local tools
- **THEN** tool definitions SHALL live under the ai/tools directory (e.g. `src/ai/tools/` with an index or per-tool modules)

### Requirement: Sessions endpoint

The backend SHALL provide `GET /api/ai/sessions` that returns the current user's chat sessions. The endpoint SHALL require a valid JWT (via `jwtAuth` middleware). The response SHALL contain only sessions whose user_id matches the authenticated user, ordered by update_time descending. Each session item SHALL include: id (string, Session Snowflake id), title (optional string), updateTime (ISO string), and llmConfigId (optional string, session-bound LLM config id).

#### Scenario: list sessions returns user sessions only
- **WHEN** a client sends `GET /api/ai/sessions` with valid JWT
- **THEN** the response SHALL contain only sessions whose user_id matches the authenticated user, ordered (e.g. by update_time descending)

#### Scenario: list sessions includes llmConfigId
- **WHEN** a client sends `GET /api/ai/sessions` with valid JWT
- **THEN** each returned session item includes `llmConfigId` when the session is bound to a config, otherwise omits it (or returns null)

### Requirement: Session messages endpoint

The backend SHALL provide `GET /api/ai/sessions/:sessionId/messages` that returns messages for a session. The endpoint SHALL require a valid JWT and SHALL verify that the session belongs to the authenticated user (user_id match). If the session does not exist or belongs to a different user, the backend SHALL respond with an error (e.g. 403 or 404). Messages SHALL be returned in order (e.g. by create_time ascending), each with id (string, Message Snowflake id), role, and parts compatible with AI SDK UIMessage.

#### Scenario: get session messages returns ordered UIMessage-shaped list

- **WHEN** a client sends `GET /api/ai/sessions/:sessionId/messages` with valid JWT and the session belongs to the user
- **THEN** the response SHALL contain messages for that session in order (e.g. create_time ascending), each with id (string), role, and parts compatible with AI SDK UIMessage

#### Scenario: get session messages rejects wrong user

- **WHEN** a client sends `GET /api/ai/sessions/:sessionId/messages` and the session exists but user_id does not match the authenticated user
- **THEN** the backend SHALL respond with an error (e.g. 403 or 404)

### Requirement: Batch session deletion endpoint

The backend SHALL provide `DELETE /api/ai/sessions` that accepts a JSON body with `sessionIds` (string array, Session Snowflake ids). The endpoint SHALL require a valid JWT (via `jwtAuth` middleware) and SHALL verify that all sessions in the array belong to the authenticated user (user_id match). The endpoint SHALL delete the specified sessions and their associated messages atomically (in a transaction). If any session does not exist or belongs to a different user, the backend SHALL respond with an error (e.g. 403 or 404) and SHALL NOT delete any sessions.

#### Scenario: batch delete deletes multiple sessions
- **WHEN** a client sends `DELETE /api/ai/sessions` with valid JWT and body `{ "sessionIds": ["id1", "id2"] }` where both sessions belong to the authenticated user
- **THEN** the backend SHALL delete both sessions and all their associated messages, and SHALL return a success response (e.g. `{ "code": 200, "msg": "success", "data": { "deleted": true } }`)

#### Scenario: batch delete verifies ownership
- **WHEN** a client sends `DELETE /api/ai/sessions` with valid JWT and body `{ "sessionIds": ["id1", "id2"] }` where id1 belongs to the user but id2 belongs to a different user
- **THEN** the backend SHALL respond with an error (e.g. 403 or 404) and SHALL NOT delete any sessions

#### Scenario: batch delete deletes associated messages
- **WHEN** a client sends `DELETE /api/ai/sessions` with valid JWT and body `{ "sessionIds": ["id1"] }` where the session has associated messages
- **THEN** the backend SHALL delete the session and all messages associated with that session in a single transaction

#### Scenario: batch delete rejects empty array
- **WHEN** a client sends `DELETE /api/ai/sessions` with valid JWT and body `{ "sessionIds": [] }`
- **THEN** the backend SHALL respond with an error (e.g. 400 BadRequest) indicating that at least one session ID is required

#### Scenario: batch delete requires authentication
- **WHEN** a client sends `DELETE /api/ai/sessions` without `Authorization` header or with an invalid/expired token
- **THEN** the backend SHALL respond with HTTP 401 and a JSON body in the unified error format (e.g. code 401, msg indicating unauthorized)

### Requirement: Chat endpoint authentication

The backend SHALL require a valid JWT for `POST /api/ai/chat`. The route SHALL use the existing `jwtAuth` middleware (or equivalent). Requests without a valid `Authorization: Bearer <token>` or with an invalid/expired token SHALL receive HTTP 401 with a response body in the existing unified error format (e.g. code, msg, data).

#### Scenario: chat returns 200 when authenticated

- **WHEN** a client sends `POST /api/ai/chat` with header `Authorization: Bearer <valid-jwt>` and a valid body
- **THEN** the request is processed and the response is a UI message stream with HTTP status 200 (subject to other chat requirements)

#### Scenario: chat returns 401 when not authenticated

- **WHEN** a client sends `POST /api/ai/chat` without `Authorization` or with an invalid/expired token
- **THEN** the backend SHALL respond with HTTP 401 and a JSON body in the unified error format (e.g. code 401, msg indicating unauthorized)
